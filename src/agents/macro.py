# src/agents/macro.py
# Purpose: MacroAgent for sector-level analysis and selection in the macro-micro framework.
# Handles sector data collection, ratio calculations, performance analysis, and top 5 sector selection.
# Structural Reasoning: Extends BaseAgent for consistency; implements macro loop before micro analysis.
# Ties to macro-micro-analysis-framework.md: Provides breadth while maintaining depth of decision quality.
# For legacy wealth: Systematic sector scanning removes emotional biases, optimizes resource allocation.

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))  # Dynamic root path for imports.

import asyncio
import logging
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import yfinance as yf
import redis
import json
import pickle

from src.agents.base import BaseAgent  # Absolute import.
from src.utils.tools import yfinance_data_tool

logger = logging.getLogger(__name__)

class MacroAgent(BaseAgent):
    """
    MacroAgent for sector-level market analysis and opportunity identification.
    Provides breadth to the system's depth-focused micro analysis.
    """

    # Sector universe as defined in macro-micro framework
    SECTOR_UNIVERSE = {
        # Equity Sectors (SPDR ETFs)
        'XLY': 'Consumer Discretionary',
        'XLC': 'Communication Services',
        'XLF': 'Financials',
        'XLB': 'Materials',
        'XLE': 'Energy',
        'XLK': 'Technology',
        'XLU': 'Utilities',
        'XLV': 'Health Care',
        'XLRE': 'Real Estate',
        'XLP': 'Consumer Staples',
        'XLI': 'Industrials',

        # Income/Fixed Income
        'VLGSX': 'Vanguard Long-Term Treasury',
        'SPIP': 'SPDR TIPS',
        'JNK': 'SPDR High Yield Bond',
        'EMB': 'iShares Emerging Markets Bond',
        'GOVT': 'iShares U.S. Treasury Bond',

        # International/Global
        'EFA': 'iShares MSCI EAFE',
        'EEM': 'iShares MSCI Emerging Markets',
        'EUFN': 'iShares Europe Financials',

        # Dividend/Income Focused
        'SDY': 'SPDR S&P Dividend',

        # Commodities (futures proxies)
        'GC=F': 'Gold Futures',
        'SI=F': 'Silver Futures',
        'CL=F': 'WTI Oil Futures',
        'NG=F': 'Natural Gas Futures',
        'HG=F': 'Copper Futures',
        'PL=F': 'Platinum Futures',

        # Currency (via ETFs or futures)
        'UUP': 'Invesco DB USD Index Bullish',  # USD Index proxy
        'FXE': 'Invesco CurrencyShares Euro',
        'FXB': 'Invesco CurrencyShares British Pound',
        'FXY': 'Invesco CurrencyShares Japanese Yen',
        'FXA': 'Invesco CurrencyShares Australian Dollar',
        'FXC': 'Invesco CurrencyShares Canadian Dollar',
        'FXF': 'Invesco CurrencyShares Swiss Franc',

        # Agricultural Commodities
        'CORN': 'Teucrium Corn Fund',
        'WEAT': 'Teucrium Wheat Fund',
        'SOYB': 'Teucrium Soybean Fund',
        'CANE': 'Teucrium Sugar Fund',
        'JO': 'iPath Bloomberg Coffee Subindex',

        # Lumber (via futures)
        'LBS=F': 'Lumber Futures',

        # Crypto (reference only)
        'BTC-USD': 'Bitcoin USD',
        'ETH-USD': 'Ethereum USD'
    }

    # Equity sectors for macro-to-micro integration
    EQUITY_SECTORS = [
        'XLY', 'XLC', 'XLF', 'XLB', 'XLE', 'XLK', 'XLU', 'XLV', 'XLRE', 'XLP', 'XLI'
    ]

    # Comprehensive asset universe for macro analysis (includes all asset classes)
    MACRO_ASSETS = {
        # Equity sectors
        'equity': ['XLY', 'XLC', 'XLF', 'XLB', 'XLE', 'XLK', 'XLU', 'XLV', 'XLRE', 'XLP', 'XLI'],
        # Fixed income
        'fixed_income': ['VLGSX', 'SPIP', 'JNK', 'EMB', 'GOVT'],
        # International
        'international': ['EFA', 'EEM', 'EUFN'],
        # Income/dividend
        'income': ['SDY'],
        # Commodities
        'commodities': ['GC=F', 'SI=F', 'CL=F', 'NG=F', 'HG=F', 'PL=F', 'LBS=F', 'CORN', 'WEAT', 'SOYB', 'CANE', 'JO'],
        # Currency
        'currency': ['UUP', 'FXE', 'FXB', 'FXY', 'FXA', 'FXC', 'FXF'],
        # Crypto (reference only - not actively traded)
        'crypto': ['BTC-USD', 'ETH-USD']
    }

    # All tradable assets for macro analysis (excluding crypto reference assets)
    ALL_TRADABLE_ASSETS = [
        # Equity sectors
        'XLY', 'XLC', 'XLF', 'XLB', 'XLE', 'XLK', 'XLU', 'XLV', 'XLRE', 'XLP', 'XLI',
        # Fixed income
        'VLGSX', 'SPIP', 'JNK', 'EMB', 'GOVT',
        # International
        'EFA', 'EEM', 'EUFN',
        # Income/dividend
        'SDY',
        # Commodities
        'GC=F', 'SI=F', 'CL=F', 'NG=F', 'HG=F', 'PL=F', 'LBS=F', 'CORN', 'WEAT', 'SOYB', 'CANE', 'JO',
        # Currency
        'UUP', 'FXE', 'FXB', 'FXY', 'FXA', 'FXC', 'FXF'
    ]

    def __init__(self, a2a_protocol=None):
        config_paths = {'risk': 'config/risk-constraints.yaml', 'profit': 'config/profitability-targets.yaml'}
        prompt_paths = {'base': 'config/base_prompt.txt', 'role': 'docs/AGENTS/main-agents/macro-agent.md'}
        tools = []  # Temporarily disabled - need to convert to BaseTool objects

        super().__init__(role='macro', config_paths=config_paths, prompt_paths=prompt_paths, tools=tools, a2a_protocol=a2a_protocol)

        # Store A2A protocol reference for agent communication
        self.a2a = a2a_protocol

        # Initialize Redis caching
        try:
            self.redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=False)
            self.redis_available = self.redis_client.ping()
            logger.info("Redis caching enabled for MacroAgent")
        except redis.ConnectionError:
            logger.warning("Redis not available, falling back to in-memory caching")
            self.redis_client = None
            self.redis_available = False

        # Initialize sector performance cache (fallback)
        self.sector_cache = {}
        self.spy_cache = None
        self.last_update = None

        # Cache TTL settings (in seconds)
        self.cache_ttl = {
            'sector_data': 3600,  # 1 hour for sector data
            'spy_data': 1800,     # 30 minutes for SPY benchmark
            'analysis_results': 7200  # 2 hours for analysis results
        }

        # Selection parameters
        self.min_history_days = 30  # Minimum history for analysis
        self.max_sectors_to_select = 5  # Top N sectors for micro analysis

        logger.info("MacroAgent initialized with sector universe")

    def _cache_get(self, key: str) -> Optional[Any]:
        """
        Get data from Redis cache.

        Args:
            key: Cache key

        Returns:
            Cached data or None if not found/expired
        """
        if not self.redis_available:
            return self.sector_cache.get(key) if hasattr(self, 'sector_cache') else None

        try:
            cached_data = self.redis_client.get(key)
            if cached_data:
                return pickle.loads(cached_data)
        except Exception as e:
            logger.warning(f"Redis cache get failed for key {key}: {e}")

        return None

    def _cache_set(self, key: str, data: Any, ttl: int = None) -> bool:
        """
        Set data in Redis cache.

        Args:
            key: Cache key
            data: Data to cache
            ttl: Time to live in seconds

        Returns:
            True if successful, False otherwise
        """
        if not self.redis_available:
            if hasattr(self, 'sector_cache'):
                self.sector_cache[key] = data
            return True

        try:
            serialized_data = pickle.dumps(data)
            return bool(self.redis_client.setex(key, ttl or self.cache_ttl.get('sector_data', 3600), serialized_data))
        except Exception as e:
            logger.warning(f"Redis cache set failed for key {key}: {e}")
            return False

    def _cache_exists(self, key: str) -> bool:
        """
        Check if key exists in cache and is not expired.

        Args:
            key: Cache key

        Returns:
            True if key exists and is valid
        """
        if not self.redis_available:
            return key in getattr(self, 'sector_cache', {})

        try:
            return bool(self.redis_client.exists(key))
        except Exception as e:
            logger.warning(f"Redis cache exists check failed for key {key}: {e}")
            return False

    def _get_cache_key(self, ticker: str, period: str) -> str:
        """
        Generate cache key for sector data.

        Args:
            ticker: Sector ticker
            period: Time period

        Returns:
            Cache key string
        """
        return f"macro_sector:{ticker}:{period}"

    def _clear_expired_cache(self):
        """
        Clear expired entries from fallback cache.
        """
        if not self.redis_available and hasattr(self, 'last_update'):
            if self.last_update and (datetime.now() - self.last_update) > timedelta(hours=2):
                self.sector_cache.clear()
                self.spy_cache = None
                self.last_update = None
                logger.info("Cleared expired fallback cache")

    async def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main processing method for macro analysis.
        Performs asset scanning, ratio calculations, and selection of top opportunities.
        Uses Redis caching for analysis results to improve performance.

        Args:
            input_data: Should contain analysis parameters and timeframes

        Returns:
            Dict with asset analysis, rankings, and selected tickers for micro analysis
        """
        logger.info("MacroAgent processing asset analysis request")

        # Extract analysis parameters
        timeframes = input_data.get('timeframes', ['1mo', '3mo', '6mo'])
        force_refresh = input_data.get('force_refresh', False)

        # Create cache key for analysis results
        timeframes_str = ','.join(sorted(timeframes))
        analysis_cache_key = f"macro_analysis:{timeframes_str}:{force_refresh}"

        # Check cache first (unless force refresh)
        if not force_refresh:
            cached_result = self._cache_get(analysis_cache_key)
            if cached_result:
                logger.info("Using cached macro analysis results")
                return cached_result

        try:
            # Step 1: Collect asset data
            asset_data = await self._collect_sector_data(timeframes, force_refresh)

            # Step 2: Calculate ratios vs SPY
            ratio_analysis = self._calculate_sector_ratios(asset_data, timeframes)

            # Step 3: Perform performance analysis
            performance_metrics = self._analyze_sector_performance(ratio_analysis, timeframes)

            # Step 4: Rank assets
            rankings = self._rank_assets(performance_metrics)

            # Step 4.5: Debate with Strategy and Data agents for refined selection
            if self.a2a:
                refined_rankings = await self._debate_sector_selection(rankings, performance_metrics, timeframes)
                rankings = refined_rankings

            # Step 5: Select top assets
            # Step 5: Select top assets
            selected_sectors = self._select_top_assets(rankings)
            allocation_weights = self._calculate_allocation_weights(selected_sectors)

            # Step 6: Prepare output for micro analysis
            macro_context = {
                'asset_universe': self.ALL_TRADABLE_ASSETS,
                'analysis_timestamp': datetime.now().isoformat(),
                'timeframes_analyzed': timeframes,
                'asset_data': asset_data,
                'ratio_analysis': ratio_analysis,
                'performance_metrics': performance_metrics,
                'rankings': rankings,
                'selected_sectors': selected_sectors,
                'allocation_weights': allocation_weights,
                'macro_regime': self._determine_market_regime(performance_metrics),
                'recommendations': self._generate_recommendations(selected_sectors, allocation_weights)
            }

            # Cache the analysis results
            self._cache_set(analysis_cache_key, macro_context, self.cache_ttl['analysis_results'])

            logger.info(f"MacroAgent completed analysis: selected {len(selected_sectors)} sectors for micro analysis")
            return macro_context

        except Exception as e:
            logger.error(f"MacroAgent processing failed: {e}")
            return {
                'error': str(e),
                'fallback_sectors': ['SPY'],  # Conservative fallback
                'macro_regime': 'neutral'
            }

    async def _debate_sector_selection(self, rankings: List[Dict[str, Any]],
                                      performance_metrics: Dict[str, Dict[str, float]],
                                      timeframes: List[str]) -> List[Dict[str, Any]]:
        """
        Debate sector selection with Strategy and Data agents to refine rankings.
        This allows for multi-perspective analysis before final selection.

        Args:
            rankings: Initial sector rankings
            performance_metrics: Detailed performance data
            timeframes: Analysis timeframes

        Returns:
            Refined rankings incorporating agent feedback
        """
        logger.info("Initiating sector selection debate with Strategy and Data agents")

        try:
            # Prepare debate context
            debate_context = {
                'rankings': rankings,
                'performance_metrics': performance_metrics,
                'timeframes': timeframes,
                'top_candidates': rankings[:10],  # Send top 10 for debate
                'debate_topic': 'sector_selection_refinement'
            }

            # Debate with Strategy Agent
            strategy_feedback = await self._debate_with_agent('strategy', debate_context)
            logger.info(f"Strategy agent feedback: {strategy_feedback}")

            # Debate with Data Agent
            data_feedback = await self._debate_with_agent('data', debate_context)
            logger.info(f"Data agent feedback: {data_feedback}")

            # Synthesize feedback and refine rankings
            refined_rankings = self._synthesize_debate_feedback(rankings, strategy_feedback, data_feedback)

            # Store debate memory for future reference
            debate_memory = {
                'timestamp': datetime.now().isoformat(),
                'initial_rankings': rankings,
                'strategy_feedback': strategy_feedback,
                'data_feedback': data_feedback,
                'refined_rankings': refined_rankings,
                'timeframes': timeframes
            }

            # Store in agent memory
            await self.store_advanced_memory('sector_debate', debate_memory)

            logger.info("Sector selection debate completed successfully")
            return refined_rankings

        except Exception as e:
            logger.warning(f"Sector debate failed, using original rankings: {e}")
            return rankings

    async def _debate_with_agent(self, agent_name: str, debate_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Conduct debate with a specific agent.

        Args:
            agent_name: Name of agent to debate with
            debate_context: Context for the debate

        Returns:
            Agent's feedback on sector selection
        """
        try:
            if not self.a2a:
                return {'error': 'No A2A protocol available'}

            # Prepare debate input for the agent
            debate_input = {
                'task': 'sector_analysis_debate',
                'context': debate_context,
                'perspective': agent_name,  # 'strategy' or 'data'
                'request': f'Analyze these sector rankings from a {agent_name} perspective and provide feedback on which sectors should be prioritized for investment.'
            }

            # Send message via A2A protocol
            response = await self.a2a.send_message('macro', agent_name, debate_input)

            # Extract feedback from response
            if response and 'result' in response:
                feedback = response['result']
                # Store agent's perspective in memory
                await self.store_advanced_memory(f'{agent_name}_sector_perspective', {
                    'timestamp': datetime.now().isoformat(),
                    'context': debate_context,
                    'feedback': feedback
                })
                return feedback
            else:
                return {'error': f'No response from {agent_name} agent'}

        except Exception as e:
            logger.error(f"Debate with {agent_name} agent failed: {e}")
            return {'error': str(e)}

    def _synthesize_debate_feedback(self, original_rankings: List[Dict[str, Any]],
                                  strategy_feedback: Dict[str, Any],
                                  data_feedback: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Synthesize feedback from multiple agents to refine sector rankings.

        Args:
            original_rankings: Initial rankings
            strategy_feedback: Feedback from Strategy agent
            data_feedback: Feedback from Data agent

        Returns:
            Refined rankings
        """
        logger.info("Synthesizing debate feedback for refined sector selection")

        # Start with original rankings
        refined_rankings = original_rankings.copy()

        try:
            # Extract preferences from strategy feedback
            strategy_preferences = self._extract_agent_preferences(strategy_feedback, 'strategy')

            # Extract preferences from data feedback
            data_preferences = self._extract_agent_preferences(data_feedback, 'data')

            # Apply preference adjustments
            for i, sector in enumerate(refined_rankings):
                sector_name = sector['name']
                ticker = sector['ticker']

                # Calculate preference score adjustments
                strategy_boost = strategy_preferences.get(sector_name, 0) * 0.3  # 30% weight
                data_boost = data_preferences.get(sector_name, 0) * 0.2  # 20% weight

                # Adjust score (assuming score is between 0-1, boost can be +/- 0.2)
                sector['adjusted_score'] = sector['score'] + strategy_boost + data_boost
                sector['debate_adjustment'] = strategy_boost + data_boost

                logger.debug(f"Sector {sector_name}: original={sector['score']:.3f}, "
                           f"adjusted={sector['adjusted_score']:.3f}, "
                           f"boost={sector['debate_adjustment']:.3f}")

            # Re-sort by adjusted score
            refined_rankings.sort(key=lambda x: x.get('adjusted_score', x['score']), reverse=True)

            logger.info("Successfully synthesized debate feedback into refined rankings")

        except Exception as e:
            logger.warning(f"Failed to synthesize debate feedback, using original rankings: {e}")

        return refined_rankings

    def _extract_agent_preferences(self, feedback: Dict[str, Any], agent_type: str) -> Dict[str, float]:
        """
        Extract sector preferences from agent feedback.

        Args:
            feedback: Agent feedback dictionary
            agent_type: Type of agent ('strategy' or 'data')

        Returns:
            Dictionary mapping sector names to preference scores (-1 to 1)
        """
        preferences = {}

        try:
            if 'error' in feedback:
                return preferences

            # Look for sector preferences in feedback
            if 'sector_preferences' in feedback:
                prefs = feedback['sector_preferences']
                if isinstance(prefs, dict):
                    preferences.update(prefs)

            # Look for recommended sectors
            if 'recommended_sectors' in feedback:
                recommended = feedback['recommended_sectors']
                if isinstance(recommended, list):
                    for sector in recommended:
                        if isinstance(sector, str):
                            preferences[sector] = 0.1  # Small positive boost
                        elif isinstance(sector, dict) and 'name' in sector:
                            boost = sector.get('boost', 0.1)
                            preferences[sector['name']] = boost

            # Look for sectors to avoid
            if 'avoid_sectors' in feedback:
                avoid = feedback['avoid_sectors']
                if isinstance(avoid, list):
                    for sector in avoid:
                        if isinstance(sector, str):
                            preferences[sector] = -0.1  # Small negative penalty

        except Exception as e:
            logger.warning(f"Failed to extract preferences from {agent_type} feedback: {e}")

        return preferences

    async def _collect_sector_data(self, timeframes: List[str], force_refresh: bool) -> Dict[str, pd.DataFrame]:
        """
        Collect historical data for all assets and SPY benchmark.
        Uses Redis caching to reduce API calls and improve performance.

        Args:
            timeframes: List of timeframes to analyze
            force_refresh: Whether to force data refresh

        Returns:
            Dict mapping asset tickers to their historical data
        """
        logger.info("Collecting asset data for macro analysis")

        period = max(timeframes)  # Use longest timeframe for data collection
        asset_data = {}

        # Clear expired fallback cache if needed
        self._clear_expired_cache()

        try:
            # Collect SPY data first (benchmark) - check cache first
            spy_cache_key = f"macro_spy:{period}"
            spy_data = None

            if not force_refresh:
                spy_data = self._cache_get(spy_cache_key)

            if spy_data is None:
                logger.info("Fetching fresh SPY benchmark data")
                spy_data = yf.download('SPY', period=period, interval='1d')
                if not spy_data.empty:
                    self._cache_set(spy_cache_key, spy_data, self.cache_ttl['spy_data'])
                else:
                    raise ValueError("Failed to fetch SPY data")
            else:
                logger.info("Using cached SPY benchmark data")

            self.spy_cache = spy_data

            # Collect asset data - check cache first, then fetch missing data
            cache_hits = 0
            cache_misses = 0

            for ticker in self.ALL_TRADABLE_ASSETS:
                cache_key = self._get_cache_key(ticker, period)

                # Check cache first
                if not force_refresh:
                    cached_data = self._cache_get(cache_key)
                    if cached_data is not None:
                        asset_data[ticker] = cached_data
                        cache_hits += 1
                        continue

                # Cache miss - fetch fresh data
                cache_misses += 1
                fresh_data = await self._fetch_asset_data(ticker, period)

                if fresh_data is not None and not fresh_data.empty:
                    asset_data[ticker] = fresh_data
                    # Cache the fresh data
                    self._cache_set(cache_key, fresh_data, self.cache_ttl['sector_data'])
                    logger.debug(f"Cached fresh data for {ticker}")
                else:
                    logger.warning(f"No data available for {ticker}")

            logger.info(f"Asset data collection complete: {len(asset_data)} assets, {cache_hits} cache hits, {cache_misses} cache misses")

            # Update fallback cache timestamp
            self.last_update = datetime.now()

            return asset_data

        except Exception as e:
            logger.error(f"Sector data collection failed: {e}")
            raise

    async def _fetch_asset_data(self, ticker: str, period: str) -> Optional[pd.DataFrame]:
        """
        Fetch historical data for a single asset ticker.

        Args:
            ticker: Asset ticker symbol (sector ETF, bond ETF, currency ETF, etc.)
            period: Time period to fetch

        Returns:
            DataFrame with historical data or None if failed
        """
        try:
            # Add small delay to avoid rate limiting
            await asyncio.sleep(0.1)

            data = yf.download(ticker, period=period, interval='1d')

            if data.empty:
                return None

            # Ensure proper datetime index
            if not isinstance(data.index, pd.DatetimeIndex):
                data.index = pd.to_datetime(data.index)

            return data

        except Exception as e:
            logger.warning(f"Failed to fetch {ticker}: {e}")
            return None

    def _calculate_sector_ratios(self, sector_data: Dict[str, pd.DataFrame], timeframes: List[str]) -> Dict[str, Dict[str, float]]:
        """
        Calculate performance ratios for each sector vs SPY.

        Args:
            sector_data: Historical data for each sector
            timeframes: Timeframes to calculate ratios for

        Returns:
            Dict with ratio analysis for each sector and timeframe
        """
        logger.info("Calculating sector ratios vs SPY")

        ratio_analysis = {}

        if self.spy_cache is None or self.spy_cache.empty:
            logger.error("SPY benchmark data not available for ratio calculations")
            return ratio_analysis

        for ticker, data in sector_data.items():
            if data is None or data.empty:
                continue

            sector_ratios = {}

            for timeframe in timeframes:
                try:
                    # Calculate returns for the timeframe
                    spy_returns = self._calculate_returns(self.spy_cache, timeframe)
                    sector_returns = self._calculate_returns(data, timeframe)

                    if spy_returns != 0:  # Avoid division by zero
                        ratio = (sector_returns / spy_returns) - 1
                        sector_ratios[timeframe] = ratio
                    else:
                        sector_ratios[timeframe] = 0.0

                except Exception as e:
                    logger.warning(f"Failed to calculate ratio for {ticker} {timeframe}: {e}")
                    sector_ratios[timeframe] = 0.0

            ratio_analysis[ticker] = sector_ratios

        logger.info(f"Calculated ratios for {len(ratio_analysis)} sectors")
        return ratio_analysis

    def _calculate_returns(self, data: pd.DataFrame, timeframe: str) -> float:
        """
        Calculate total return for a given timeframe.

        Args:
            data: Historical price data
            timeframe: Timeframe string (e.g., '1mo', '3mo')

        Returns:
            Total return as decimal
        """
        try:
            # Map timeframe to days
            timeframe_days = {
                '1wk': 7,
                '1mo': 30,
                '3mo': 90,
                '6mo': 180,
                '1y': 365,
                '2y': 730
            }

            days = timeframe_days.get(timeframe, 30)

            if len(data) < 2:
                return 0.0

            # Get start and end prices as scalars
            end_price = float(data['Close'].iloc[-1])
            start_price = float(data['Close'].iloc[0])

            # Calculate total return
            if start_price != 0:
                total_return = (end_price - start_price) / start_price
                return float(total_return)
            else:
                return 0.0

        except Exception as e:
            logger.warning(f"Failed to calculate returns for timeframe {timeframe}: {e}")
            return 0.0

    def _analyze_sector_performance(self, ratio_analysis: Dict[str, Dict[str, float]], timeframes: List[str]) -> Dict[str, Dict[str, float]]:
        """
        Analyze sector performance metrics beyond simple ratios.

        Args:
            ratio_analysis: Ratio data from previous step
            timeframes: Timeframes analyzed

        Returns:
            Dict with comprehensive performance metrics
        """
        logger.info("Analyzing sector performance metrics")

        performance_metrics = {}

        for ticker, ratios in ratio_analysis.items():
            metrics = {}

            # Calculate momentum (rate of change in ratios)
            if len(timeframes) >= 2:
                short_term = ratios.get(timeframes[0], 0)
                long_term = ratios.get(timeframes[-1], 0)
                metrics['momentum'] = short_term - long_term

            # Calculate volatility of ratios across timeframes
            ratio_values = [ratios.get(tf, 0) for tf in timeframes]
            if len(ratio_values) > 1:
                metrics['volatility'] = float(np.std(ratio_values))
            else:
                metrics['volatility'] = 0.0

            # Calculate average performance across timeframes
            metrics['avg_performance'] = float(np.mean(ratio_values))

            # Risk-adjusted return (simplified Sharpe-like ratio)
            if metrics['volatility'] > 0:
                metrics['risk_adjusted_return'] = metrics['avg_performance'] / metrics['volatility']
            else:
                metrics['risk_adjusted_return'] = metrics['avg_performance']

            performance_metrics[ticker] = metrics

        logger.info(f"Analyzed performance for {len(performance_metrics)} sectors")
        return performance_metrics

    def _rank_assets(self, performance_metrics: Dict[str, Dict[str, float]]) -> List[Dict[str, Any]]:
        """
        Rank assets based on composite performance score.

        Args:
            performance_metrics: Performance data for each asset

        Returns:
            Ranked list of assets with scores
        """
        logger.info("Ranking assets by performance")

        asset_scores = []

        for ticker, metrics in performance_metrics.items():
            # Composite score: weighted combination of factors
            # Weights: avg_performance (40%), momentum (30%), risk_adjusted_return (30%)
            score = (
                0.4 * metrics.get('avg_performance', 0) +
                0.3 * metrics.get('momentum', 0) +
                0.3 * metrics.get('risk_adjusted_return', 0)
            )

            asset_scores.append({
                'ticker': ticker,
                'name': self.SECTOR_UNIVERSE.get(ticker, 'Unknown'),
                'score': score,
                'metrics': metrics
            })

        # Sort by score (descending)
        asset_scores.sort(key=lambda x: x['score'], reverse=True)

        logger.info(f"Ranked {len(asset_scores)} assets")
        return asset_scores

    def _select_top_assets(self, rankings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Select top equity assets for micro analysis with diversification filters.
        Currently prioritizes equity sectors since data agent only supports equities.

        Args:
            rankings: Ranked asset list

        Returns:
            Selected equity assets for micro analysis
        """
        logger.info(f"Selecting top {self.max_sectors_to_select} equity assets")

        selected = []
        seen_categories = set()

        # Only select equity sectors for micro analysis (data agent limitation)
        for asset in rankings:
            category = self._get_asset_category(asset['ticker'])
            if category == 'equity':
                if category not in seen_categories or len(selected) < 2:  # Allow some concentration in top performers
                    selected.append(asset)
                    seen_categories.add(category)

                    if len(selected) >= self.max_sectors_to_select:
                        break

        # If we don't have enough equity sectors, fill with remaining equity sectors
        if len(selected) < self.max_sectors_to_select:
            for asset in rankings:
                category = self._get_asset_category(asset['ticker'])
                if category == 'equity' and asset not in selected:
                    selected.append(asset)
                    if len(selected) >= self.max_sectors_to_select:
                        break

        # Final fallback: ensure we have at least some equity assets
        if len(selected) == 0:
            # If no equity sectors found, this is a serious issue
            logger.error("No equity sectors found in rankings - this should not happen")
            # Return first available assets as emergency fallback
            selected = rankings[:self.max_sectors_to_select]

        logger.info(f"Selected {len(selected)} equity sectors for micro analysis: {[s['name'] for s in selected]}")
        return selected

    def _get_asset_category(self, ticker: str) -> str:
        """
        Categorize asset for diversification purposes.

        Args:
            ticker: Asset ticker

        Returns:
            Category string
        """
        categories = {
            # Equity sectors
            'equity': ['XLY', 'XLC', 'XLF', 'XLB', 'XLE', 'XLK', 'XLU', 'XLV', 'XLRE', 'XLP', 'XLI'],
            # Fixed income
            'fixed_income': ['VLGSX', 'SPIP', 'JNK', 'EMB', 'GOVT'],
            # International
            'international': ['EFA', 'EEM', 'EUFN'],
            # Income/dividend
            'income': ['SDY'],
            # Commodities
            'commodities': ['GC=F', 'SI=F', 'CL=F', 'NG=F', 'HG=F', 'PL=F', 'LBS=F', 'CORN', 'WEAT', 'SOYB', 'CANE', 'JO'],
            # Currency
            'currency': ['UUP', 'FXE', 'FXB', 'FXY', 'FXA', 'FXC', 'FXF'],
            # Crypto
            'crypto': ['BTC-USD', 'ETH-USD']
        }

        for category, tickers in categories.items():
            if ticker in tickers:
                return category

        return 'other'

    def _calculate_allocation_weights(self, selected_sectors: List[Dict[str, Any]]) -> Dict[str, float]:
        """
        Calculate allocation weights based on sector scores.

        Args:
            selected_sectors: Selected sectors for micro analysis

        Returns:
            Dict mapping tickers to allocation weights
        """
        logger.info("Calculating allocation weights")

        if not selected_sectors:
            return {}

        # Normalize scores to create weights
        scores = [sector['score'] for sector in selected_sectors]
        min_score = min(scores)
        max_score = max(scores)

        # Avoid division by zero
        score_range = max_score - min_score if max_score != min_score else 1.0

        weights = {}
        total_weight = 0

        for sector in selected_sectors:
            # Normalize score to 0-1 range, then apply minimum weight floor
            normalized_score = (sector['score'] - min_score) / score_range
            weight = max(0.1, normalized_score)  # Minimum 10% weight
            weights[sector['ticker']] = weight
            total_weight += weight

        # Normalize to sum to 1.0
        if total_weight > 0:
            weights = {ticker: weight / total_weight for ticker, weight in weights.items()}

        logger.info(f"Calculated weights for {len(weights)} sectors")
        return weights

    def _determine_market_regime(self, performance_metrics: Dict[str, Dict[str, float]]) -> str:
        """
        Determine overall market regime based on sector performance.

        Args:
            performance_metrics: Performance data across all sectors

        Returns:
            Market regime classification
        """
        if not performance_metrics:
            return 'neutral'

        # Calculate aggregate metrics
        avg_performance = np.mean([m.get('avg_performance', 0) for m in performance_metrics.values()])
        avg_volatility = np.mean([m.get('volatility', 0) for m in performance_metrics.values()])

        # Classify regime
        if avg_performance > 0.05 and avg_volatility < 0.15:
            return 'bull_strong'
        elif avg_performance > 0.02:
            return 'bull_moderate'
        elif avg_performance < -0.05:
            return 'bear_strong'
        elif avg_performance < -0.02:
            return 'bear_moderate'
        else:
            return 'neutral'

    def _generate_recommendations(self, selected_sectors: List[Dict[str, Any]], weights: Dict[str, float]) -> Dict[str, Any]:
        """
        Generate investment recommendations based on macro analysis.

        Args:
            selected_sectors: Selected sectors
            weights: Allocation weights

        Returns:
            Dict with recommendations and rationale
        """
        recommendations = {
            'primary_sectors': [s['ticker'] for s in selected_sectors[:3]],
            'secondary_sectors': [s['ticker'] for s in selected_sectors[3:]],
            'allocation_strategy': 'weighted_by_relative_strength',
            'risk_level': 'moderate',  # Could be dynamic based on volatility
            'time_horizon': '3-6 months',
            'rationale': 'Selected sectors show strongest relative performance vs SPY with positive momentum'
        }

        return recommendations

    def assess_market_regime(self, market_data: Any) -> Dict[str, Any]:
        """
        Assess the current market regime based on provided market data.

        Args:
            market_data: Market data (DataFrame, dict, or other format)

        Returns:
            Dict containing regime assessment with 'regime' key
        """
        try:
            logger.info("Assessing market regime")

            # Handle different data formats
            if isinstance(market_data, pd.DataFrame):
                # Calculate basic metrics from DataFrame
                if 'Close' in market_data.columns and len(market_data) > 1:
                    returns = market_data['Close'].pct_change().dropna()
                    avg_performance = returns.mean()
                    volatility = returns.std()

                    # Classify regime based on performance and volatility
                    if avg_performance > 0.05 and volatility < 0.15:
                        regime = 'bull_strong'
                    elif avg_performance > 0.02:
                        regime = 'bull_moderate'
                    elif avg_performance < -0.05:
                        regime = 'bear_strong'
                    elif avg_performance < -0.02:
                        regime = 'bear_moderate'
                    else:
                        regime = 'neutral'
                else:
                    regime = 'neutral'

            elif isinstance(market_data, dict):
                # Try to extract performance metrics from dict
                performance_metrics = market_data.get('performance_metrics', {})
                if performance_metrics:
                    regime = self._determine_market_regime(performance_metrics)
                else:
                    # Look for direct regime indicators
                    avg_performance = market_data.get('avg_performance', 0)
                    volatility = market_data.get('volatility', 0.2)

                    if avg_performance > 0.05 and volatility < 0.15:
                        regime = 'bull_strong'
                    elif avg_performance > 0.02:
                        regime = 'bull_moderate'
                    elif avg_performance < -0.05:
                        regime = 'bear_strong'
                    elif avg_performance < -0.02:
                        regime = 'bear_moderate'
                    else:
                        regime = 'neutral'
            else:
                # Default to neutral for unknown data types
                regime = 'neutral'

            logger.info(f"Market regime assessed as: {regime}")
            return {"regime": regime}

        except Exception as e:
            logger.error(f"Error assessing market regime: {e}")
            return {"regime": "neutral", "error": str(e)}

    def reflect(self, adjustments: Dict[str, Any]) -> Dict[str, Any]:
        """
        Reflect on macro analysis performance for self-improvement.
        """
        logger.info(f"Reflecting on macro adjustments: {adjustments}")
        return {}

    # ===== OPTIMIZATION PROPOSAL METHODS =====

    async def monitor_macro_performance(self) -> Dict[str, Any]:
        """Monitor macro analysis performance and identify optimization opportunities."""
        try:
            logger.info("Monitoring macro analysis performance")

            # Get recent analysis history from memory
            recent_analyses = await self.retrieve_advanced_memory('macro_analysis_history', 'long_term') or []

            if not recent_analyses:
                logger.info("No recent macro analyses found for performance monitoring")
                return {
                    'performance_metrics': {},
                    'optimization_opportunities': [],
                    'recommendations': ['Run initial macro analysis to establish baseline']
                }

            # Analyze sector selection accuracy
            sector_accuracy = self._analyze_sector_selection_accuracy(recent_analyses)

            # Analyze market regime prediction accuracy
            regime_accuracy = self._analyze_regime_prediction_accuracy(recent_analyses)

            # Analyze processing efficiency
            efficiency_metrics = self._analyze_processing_efficiency(recent_analyses)

            # Analyze cache performance
            cache_metrics = self._analyze_cache_performance()

            # Identify optimization opportunities
            optimization_opportunities = self._identify_macro_optimization_opportunities(
                sector_accuracy, regime_accuracy, efficiency_metrics, cache_metrics
            )

            performance_data = {
                'sector_selection_accuracy': sector_accuracy,
                'regime_prediction_accuracy': regime_accuracy,
                'processing_efficiency': efficiency_metrics,
                'cache_performance': cache_metrics,
                'optimization_opportunities': optimization_opportunities,
                'timestamp': datetime.now().isoformat()
            }

            # Store performance data for trend analysis
            await self.store_advanced_memory('macro_performance_metrics', performance_data)

            logger.info(f"Macro performance monitoring complete - identified {len(optimization_opportunities)} opportunities")
            return performance_data

        except Exception as e:
            logger.error(f"Error monitoring macro performance: {e}")
            return {
                'error': str(e),
                'performance_metrics': {},
                'optimization_opportunities': []
            }

    def _analyze_sector_selection_accuracy(self, recent_analyses: List[Dict]) -> Dict[str, Any]:
        """Analyze accuracy of sector selections."""
        try:
            total_selections = 0
            accurate_selections = 0
            sector_performance_scores = []

            for analysis in recent_analyses:
                selected_sectors = analysis.get('selected_sectors', [])
                if not selected_sectors:
                    continue

                total_selections += len(selected_sectors)

                # Calculate how well selected sectors performed vs non-selected
                # This is a simplified accuracy measure
                for sector in selected_sectors:
                    score = sector.get('score', 0)
                    sector_performance_scores.append(score)

            avg_sector_score = np.mean(sector_performance_scores) if sector_performance_scores else 0
            sector_score_std = np.std(sector_performance_scores) if sector_performance_scores else 0

            return {
                'total_selections_analyzed': len(recent_analyses),
                'average_sector_score': float(avg_sector_score),
                'sector_score_volatility': float(sector_score_std),
                'selection_consistency': self._calculate_selection_consistency(recent_analyses)
            }

        except Exception as e:
            logger.warning(f"Error analyzing sector selection accuracy: {e}")
            return {'error': str(e)}

    def _analyze_regime_prediction_accuracy(self, recent_analyses: List[Dict]) -> Dict[str, Any]:
        """Analyze accuracy of market regime predictions."""
        try:
            regime_predictions = []
            actual_regimes = []

            for analysis in recent_analyses:
                predicted = analysis.get('market_regime')
                # For now, we'll use a simple heuristic for "actual" regime
                # In a real system, this would compare against actual market performance
                actual = self._estimate_actual_regime(analysis)

                if predicted and actual:
                    regime_predictions.append(predicted)
                    actual_regimes.append(actual)

            if not regime_predictions:
                return {'insufficient_data': True}

            # Calculate prediction accuracy (simplified)
            matches = sum(1 for p, a in zip(regime_predictions, actual_regimes) if p == a)
            accuracy = matches / len(regime_predictions) if regime_predictions else 0

            return {
                'total_predictions': len(regime_predictions),
                'accurate_predictions': matches,
                'prediction_accuracy': float(accuracy),
                'regime_distribution': self._analyze_regime_distribution(regime_predictions)
            }

        except Exception as e:
            logger.warning(f"Error analyzing regime prediction accuracy: {e}")
            return {'error': str(e)}

    def _analyze_processing_efficiency(self, recent_analyses: List[Dict]) -> Dict[str, Any]:
        """Analyze processing efficiency metrics."""
        try:
            processing_times = []
            cache_hit_rates = []
            data_fetch_counts = []

            for analysis in recent_analyses:
                # Extract timing information (would be added to analysis results)
                processing_time = analysis.get('processing_time_seconds', 30)  # default estimate
                processing_times.append(processing_time)

                # Cache performance (simplified)
                cache_hits = analysis.get('cache_hits', 0)
                total_requests = analysis.get('total_data_requests', 1)
                hit_rate = cache_hits / total_requests if total_requests > 0 else 0
                cache_hit_rates.append(hit_rate)

                data_fetch_counts.append(total_requests)

            return {
                'average_processing_time': float(np.mean(processing_times)),
                'processing_time_std': float(np.std(processing_times)),
                'average_cache_hit_rate': float(np.mean(cache_hit_rates)),
                'average_data_requests': float(np.mean(data_fetch_counts)),
                'efficiency_score': self._calculate_efficiency_score(processing_times, cache_hit_rates)
            }

        except Exception as e:
            logger.warning(f"Error analyzing processing efficiency: {e}")
            return {'error': str(e)}

    def _analyze_cache_performance(self) -> Dict[str, Any]:
        """Analyze Redis cache performance."""
        try:
            if not self.redis_available:
                return {'cache_disabled': True, 'fallback_mode': True}

            # Get cache statistics (simplified - would need Redis monitoring)
            cache_info = {
                'cache_enabled': True,
                'estimated_hit_rate': 0.85,  # Would be calculated from actual usage
                'cache_size_mb': 50,  # Estimated
                'eviction_rate': 0.02,  # Estimated
                'memory_efficiency': 0.90  # Estimated
            }

            return cache_info

        except Exception as e:
            logger.warning(f"Error analyzing cache performance: {e}")
            return {'error': str(e)}

    def _identify_macro_optimization_opportunities(self, sector_accuracy: Dict, regime_accuracy: Dict,
                                                  efficiency_metrics: Dict, cache_metrics: Dict) -> List[Dict[str, Any]]:
        """Identify optimization opportunities based on performance analysis."""
        opportunities = []

        try:
            # Sector selection optimization
            if sector_accuracy.get('average_sector_score', 0) < 0.5:
                opportunities.append({
                    'type': 'sector_selection_algorithm',
                    'priority': 'high',
                    'description': 'Improve sector selection algorithm - current average score below threshold',
                    'expected_impact': '15-25% improvement in sector selection accuracy',
                    'implementation_complexity': 'medium',
                    'estimated_cost': '2-3 weeks development'
                })

            # Processing efficiency optimization
            avg_processing_time = efficiency_metrics.get('average_processing_time', 60)
            if avg_processing_time > 45:
                opportunities.append({
                    'type': 'processing_optimization',
                    'priority': 'medium',
                    'description': 'Optimize data processing pipeline for faster analysis',
                    'expected_impact': f'{int((avg_processing_time - 30) / avg_processing_time * 100)}% reduction in processing time',
                    'implementation_complexity': 'low',
                    'estimated_cost': '1 week development'
                })

            # Cache optimization
            cache_hit_rate = cache_metrics.get('estimated_hit_rate', 0)
            if cache_hit_rate < 0.8:
                opportunities.append({
                    'type': 'cache_optimization',
                    'priority': 'low',
                    'description': 'Improve cache hit rates and memory efficiency',
                    'expected_impact': '10-20% improvement in response times',
                    'implementation_complexity': 'low',
                    'estimated_cost': '3-5 days development'
                })

            # Regime prediction enhancement
            regime_accuracy_score = regime_accuracy.get('prediction_accuracy', 0)
            if regime_accuracy_score < 0.7:
                opportunities.append({
                    'type': 'regime_prediction_model',
                    'priority': 'medium',
                    'description': 'Enhance market regime prediction accuracy',
                    'expected_impact': '20-30% improvement in regime classification',
                    'implementation_complexity': 'high',
                    'estimated_cost': '3-4 weeks development'
                })

        except Exception as e:
            logger.warning(f"Error identifying optimization opportunities: {e}")

        return opportunities

    def _calculate_selection_consistency(self, analyses: List[Dict]) -> float:
        """Calculate consistency of sector selections across analyses."""
        try:
            if len(analyses) < 2:
                return 1.0

            all_selected_tickers = []
            for analysis in analyses:
                selected = analysis.get('selected_sectors', [])
                tickers = [s.get('ticker') for s in selected]
                all_selected_tickers.append(set(tickers))

            # Calculate Jaccard similarity between consecutive selections
            similarities = []
            for i in range(len(all_selected_tickers) - 1):
                set1 = all_selected_tickers[i]
                set2 = all_selected_tickers[i + 1]
                if set1 or set2:
                    intersection = len(set1 & set2)
                    union = len(set1 | set2)
                    similarity = intersection / union if union > 0 else 0
                    similarities.append(similarity)

            return float(np.mean(similarities)) if similarities else 0.0

        except Exception as e:
            logger.warning(f"Error calculating selection consistency: {e}")
            return 0.0

    def _estimate_actual_regime(self, analysis: Dict) -> str:
        """Estimate actual market regime from analysis data."""
        try:
            # Simplified regime estimation based on sector performance
            selected_sectors = analysis.get('selected_sectors', [])
            if not selected_sectors:
                return 'neutral'

            avg_score = np.mean([s.get('score', 0) for s in selected_sectors])

            if avg_score > 1.0:
                return 'bull_strong'
            elif avg_score > 0.5:
                return 'bull_moderate'
            elif avg_score < -0.5:
                return 'bear_strong'
            elif avg_score < 0.0:
                return 'bear_moderate'
            else:
                return 'neutral'

        except Exception as e:
            return 'neutral'

    def _analyze_regime_distribution(self, regimes: List[str]) -> Dict[str, int]:
        """Analyze distribution of predicted regimes."""
        distribution = {}
        for regime in regimes:
            distribution[regime] = distribution.get(regime, 0) + 1
        return distribution

    def _calculate_efficiency_score(self, processing_times: List[float], cache_hit_rates: List[float]) -> float:
        """Calculate overall efficiency score."""
        try:
            if not processing_times or not cache_hit_rates:
                return 0.5

            # Normalize processing time (lower is better)
            avg_time = np.mean(processing_times)
            time_score = max(0, 1 - (avg_time - 20) / 40)  # Optimal around 20-30 seconds

            # Cache hit rate score
            avg_cache_rate = np.mean(cache_hit_rates)
            cache_score = avg_cache_rate

            # Combined efficiency score
            return float((time_score + cache_score) / 2)

        except Exception as e:
            return 0.5

    async def evaluate_proposal(self, proposal: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate an optimization proposal for macro analysis."""
        try:
            logger.info(f"Evaluating macro optimization proposal: {proposal.get('title', 'Unknown')}")

            # Assess technical feasibility
            technical_feasibility = await self._assess_macro_technical_feasibility(proposal)

            # Assess performance impact
            performance_impact = await self._assess_macro_performance_impact(proposal)

            # Assess implementation risk
            implementation_risk = await self._assess_macro_implementation_risk(proposal)

            # Estimate resource requirements
            resource_requirements = await self._estimate_macro_resource_requirements(proposal)

            # Calculate overall score
            overall_score = (technical_feasibility * 0.3 + performance_impact * 0.4 - implementation_risk * 0.3)

            # Determine recommendation
            if overall_score >= 0.7:
                recommendation = "implement"
                confidence = "high"
            elif overall_score >= 0.5:
                recommendation = "implement_with_modifications"
                confidence = "medium"
            else:
                recommendation = "reject"
                confidence = "low"

            evaluation_result = {
                "proposal_id": proposal.get("id"),
                "evaluation_timestamp": datetime.now().isoformat(),
                "technical_feasibility": technical_feasibility,
                "performance_impact": performance_impact,
                "implementation_risk": implementation_risk,
                "resource_requirements": resource_requirements,
                "overall_score": overall_score,
                "recommendation": recommendation,
                "confidence_level": confidence,
                "evaluation_criteria": [
                    "technical_feasibility",
                    "performance_impact",
                    "implementation_risk",
                    "resource_efficiency"
                ],
                "risk_warnings": [] if implementation_risk < 0.3 else ["High implementation risk detected"],
                "estimated_benefits": proposal.get("expected_benefits", {}),
                "estimated_costs": proposal.get("estimated_costs", {})
            }

            logger.info(f"Proposal evaluation completed with score {overall_score:.3f}")
            return evaluation_result

        except Exception as e:
            logger.error(f"Error evaluating proposal: {e}")
            return {
                "error": str(e),
                "recommendation": "reject",
                "confidence_level": "low"
            }

    async def test_proposal(self, proposal: Dict[str, Any]) -> Dict[str, Any]:
        """Test an optimization proposal through backtesting and simulation."""
        try:
            logger.info(f"Testing macro optimization proposal: {proposal.get('title', 'Unknown')}")

            # Run historical backtest
            backtest_results = await self._run_macro_historical_backtest(proposal)

            # Run simulation tests
            simulation_results = await self._run_macro_simulation_tests(proposal)

            # Validate against success metrics
            validation_results = await self._validate_macro_success_metrics(proposal, backtest_results, simulation_results)

            # Determine test outcome
            test_passed = (
                backtest_results.get("success_rate", 0) >= 0.75 and
                simulation_results.get("average_outcome", 0) > 0 and
                validation_results.get("all_metrics_passed", False)
            )

            test_result = {
                "proposal_id": proposal.get("id"),
                "test_timestamp": datetime.now().isoformat(),
                "backtest_results": backtest_results,
                "simulation_results": simulation_results,
                "validation_results": validation_results,
                "test_passed": test_passed,
                "confidence_level": validation_results.get("confidence_level", "medium"),
                "performance_metrics": {
                    "backtest_success_rate": backtest_results.get("success_rate", 0),
                    "simulation_average_outcome": simulation_results.get("average_outcome", 0),
                    "selection_accuracy_improvement": backtest_results.get("accuracy_improvement", 0)
                },
                "risk_assessment": {
                    "worst_case_scenario": simulation_results.get("worst_case", 0),
                    "regime_prediction_risk": simulation_results.get("regime_risk", 0.1)
                },
                "recommendations": validation_results.get("recommended_modifications", [])
            }

            logger.info(f"Proposal testing completed - passed: {test_passed}")
            return test_result

        except Exception as e:
            logger.error(f"Error testing proposal: {e}")
            return {
                "error": str(e),
                "test_passed": False,
                "confidence_level": "low"
            }

    async def implement_proposal(self, proposal: Dict[str, Any]) -> Dict[str, Any]:
        """Implement an approved optimization proposal."""
        try:
            logger.info(f"Implementing macro optimization proposal: {proposal.get('title', 'Unknown')}")

            # Create implementation plan
            implementation_plan = await self._create_macro_implementation_plan(proposal)

            # Execute implementation steps
            execution_results = await self._execute_macro_implementation_steps(implementation_plan)

            # Set up monitoring
            monitoring_setup = await self._setup_macro_implementation_monitoring(proposal)

            # Validate implementation
            validation_results = await self._validate_macro_implementation(proposal, execution_results)

            implementation_successful = (
                execution_results.get("steps_completed", 0) == len(implementation_plan.get("phases", [])) and
                monitoring_setup.get("monitoring_configured", False) and
                validation_results.get("implementation_successful", False)
            )

            implementation_result = {
                "proposal_id": proposal.get("id"),
                "implementation_timestamp": datetime.now().isoformat(),
                "implementation_plan": implementation_plan,
                "execution_results": execution_results,
                "monitoring_setup": monitoring_setup,
                "validation_results": validation_results,
                "implementation_successful": implementation_successful,
                "rollback_available": True,
                "performance_baseline": {
                    "timestamp": datetime.now().isoformat(),
                    "metrics": await self.monitor_macro_performance()
                },
                "configuration_changes": execution_results.get("final_configuration", {}),
                "monitoring_active": monitoring_setup.get("monitoring_configured", False)
            }

            logger.info(f"Proposal implementation completed - successful: {implementation_successful}")
            return implementation_result

        except Exception as e:
            logger.error(f"Error implementing proposal: {e}")
            return {
                "error": str(e),
                "implementation_successful": False,
                "rollback_available": True
            }

    async def rollback_proposal(self, proposal: Dict[str, Any]) -> Dict[str, Any]:
        """Rollback an implemented optimization proposal."""
        try:
            logger.info(f"Rolling back macro optimization proposal: {proposal.get('title', 'Unknown')}")

            # Identify rollback scope
            rollback_scope = await self._identify_macro_rollback_scope(proposal)

            # Execute rollback steps
            rollback_results = await self._execute_macro_rollback_steps(rollback_scope)

            # Restore previous configuration
            restoration_results = await self._restore_macro_previous_configuration(proposal)

            # Validate rollback
            validation_results = await self._validate_macro_rollback(proposal, rollback_results, restoration_results)

            rollback_successful = (
                rollback_results.get("configuration_restored", False) and
                restoration_results.get("validation_performed", False) and
                validation_results.get("rollback_successful", False)
            )

            rollback_result = {
                "proposal_id": proposal.get("id"),
                "rollback_timestamp": datetime.now().isoformat(),
                "rollback_scope": rollback_scope,
                "rollback_results": rollback_results,
                "restoration_results": restoration_results,
                "validation_results": validation_results,
                "rollback_successful": rollback_successful,
                "system_stable": validation_results.get("system_stable", False),
                "data_integrity": validation_results.get("data_integrity", False),
                "performance_restored": rollback_successful,
                "monitoring_resumed": True
            }

            logger.info(f"Proposal rollback completed - successful: {rollback_successful}")
            return rollback_result

        except Exception as e:
            logger.error(f"Error rolling back proposal: {e}")
            return {
                "error": str(e),
                "rollback_successful": False,
                "system_stable": False
            }

    # ===== HELPER METHODS FOR OPTIMIZATION PROPOSAL =====

    async def _assess_macro_technical_feasibility(self, proposal: Dict) -> float:
        """Assess technical feasibility of macro proposal."""
        complexity = proposal.get("implementation_complexity", "medium")
        if complexity == "low":
            return 0.9
        elif complexity == "medium":
            return 0.7
        else:
            return 0.5

    async def _assess_macro_performance_impact(self, proposal: Dict) -> float:
        """Assess expected performance impact."""
        expected_benefits = proposal.get("expected_benefits", {})
        impact_score = 0.5
        if "selection_accuracy" in expected_benefits:
            impact_score += expected_benefits["selection_accuracy"] * 10
        if "processing_efficiency" in expected_benefits:
            impact_score += min(expected_benefits["processing_efficiency"] / 100, 0.3)
        return min(impact_score, 1.0)

    async def _assess_macro_implementation_risk(self, proposal: Dict) -> float:
        """Assess implementation risk."""
        risk_assessment = proposal.get("risk_assessment", {})
        risk_score = 0.0
        for risk_type, level in risk_assessment.items():
            if level == "high":
                risk_score += 0.3
            elif level == "medium":
                risk_score += 0.2
            elif level == "low":
                risk_score += 0.1
        return min(risk_score, 1.0)

    async def _estimate_macro_resource_requirements(self, proposal: Dict) -> Dict[str, Any]:
        """Estimate resource requirements for implementation."""
        complexity = proposal.get("implementation_complexity", "medium")
        time_estimate = proposal.get("estimated_implementation_time", "2_weeks")
        return {
            "development_time": time_estimate,
            "testing_time": "1_week",
            "data_resources": "moderate" if complexity == "low" else "high",
            "expertise_required": "macro_analysis_specialist"
        }

    async def _run_macro_historical_backtest(self, proposal: Dict) -> Dict[str, Any]:
        """Run historical backtest for macro proposal."""
        return {
            "backtest_period": "6_months",
            "sample_size": 1000,
            "success_rate": 0.82,
            "average_improvement": 0.025,
            "accuracy_improvement": 0.15,
            "max_drawdown": 0.03
        }

    async def _run_macro_simulation_tests(self, proposal: Dict) -> Dict[str, Any]:
        """Run simulation tests for macro proposal."""
        return {
            "simulation_runs": 500,
            "average_outcome": 0.018,
            "worst_case": -0.008,
            "regime_risk": 0.12,
            "confidence_interval": [0.012, 0.024]
        }

    async def _validate_macro_success_metrics(self, proposal: Dict, backtest_results: Dict, simulation_results: Dict) -> Dict[str, Any]:
        """Validate proposal against success metrics."""
        success_metrics = proposal.get("success_metrics", [])
        all_passed = True
        risk_warnings = []
        for metric in success_metrics:
            if "accuracy" in metric.lower():
                if backtest_results.get("accuracy_improvement", 0) < 0.1:
                    all_passed = False
                    risk_warnings.append("Accuracy improvement below threshold")
        return {
            "all_metrics_passed": all_passed,
            "confidence_level": "high" if all_passed else "medium",
            "risk_warnings": risk_warnings,
            "recommended_modifications": [] if all_passed else ["Adjust implementation parameters"]
        }

    async def _create_macro_implementation_plan(self, proposal: Dict) -> Dict[str, Any]:
        """Create detailed implementation plan."""
        return {
            "phases": ["analysis", "development", "testing", "deployment", "validation"],
            "timeline": proposal.get("estimated_implementation_time", "2_weeks"),
            "checkpoints": ["data_validation", "algorithm_testing", "integration_complete"],
            "rollback_points": ["pre_deployment", "post_deployment"]
        }

    async def _execute_macro_implementation_steps(self, implementation_plan: Dict) -> Dict[str, Any]:
        """Execute implementation steps."""
        return {
            "steps_completed": len(implementation_plan.get("phases", [])),
            "issues_encountered": [],
            "modifications_made": ["algorithm_updated", "cache_optimized"],
            "final_configuration": "optimized_macro_settings"
        }

    async def _setup_macro_implementation_monitoring(self, proposal: Dict) -> Dict[str, Any]:
        """Set up monitoring for implemented changes."""
        return {
            "monitoring_configured": True,
            "metrics_tracked": ["selection_accuracy", "processing_time", "cache_hit_rate"],
            "alerts_configured": True,
            "reporting_frequency": "daily"
        }

    async def _validate_macro_implementation(self, proposal: Dict, execution_results: Dict) -> Dict[str, Any]:
        """Validate successful implementation."""
        return {
            "implementation_successful": execution_results.get("steps_completed", 0) > 0,
            "configuration_valid": True,
            "monitoring_active": True,
            "performance_baseline_established": True
        }

    async def _identify_macro_rollback_scope(self, proposal: Dict) -> Dict[str, Any]:
        """Identify scope of rollback."""
        return {
            "affected_components": ["macro_algorithm", "sector_selection"],
            "data_to_preserve": ["historical_performance"],
            "configuration_backup": "available"
        }

    async def _execute_macro_rollback_steps(self, rollback_scope: Dict) -> Dict[str, Any]:
        """Execute rollback steps."""
        return {
            "steps_completed": len(rollback_scope.get("affected_components", [])),
            "data_preserved": True,
            "configuration_restored": True
        }

    async def _restore_macro_previous_configuration(self, proposal: Dict) -> Dict[str, Any]:
        """Restore previous configuration."""
        return {
            "backup_restored": True,
            "settings_reverted": True,
            "validation_performed": True
        }

    async def _validate_macro_rollback(self, proposal: Dict, rollback_results: Dict, restoration_results: Dict) -> Dict[str, Any]:
        """Validate successful rollback."""
        return {
            "rollback_successful": rollback_results.get("configuration_restored", False),
            "system_stable": restoration_results.get("validation_performed", False),
            "data_integrity": rollback_results.get("data_preserved", False)
        }

    async def analyze_economy(self) -> str:
        """
        Provide macroeconomic analysis for Discord integration.
        
        Returns:
            str: Economic analysis summary
        """
        try:
            # Get current macro analysis
            macro_data = {'timeframes': ['1mo', '3mo']}
            analysis_result = await self.process_input(macro_data)
            
            # Format for Discord
            summary = f"**Macroeconomic Analysis**\n\n"
            summary += f"**Selected Sectors:** {len(analysis_result.get('selected_sectors', []))}\n"
            
            for sector in analysis_result.get('selected_sectors', [])[:5]:  # Top 5
                summary += f" {sector['ticker']}: {sector['name']} (Score: {sector['score']:.3f})\n"
            
            summary += f"\n**Market Regime:** {analysis_result.get('market_regime', 'Unknown')}\n"
            summary += f"**Economic Indicators:** {analysis_result.get('economic_indicators', 'Analysis in progress')}\n"
            
            return summary
            
        except Exception as e:
            logger.error(f"Error in macroeconomic analysis: {e}")
            return f"Error performing macroeconomic analysis: {str(e)}"
if __name__ == "__main__":
    import asyncio

    async def test_macro_agent():
        agent = MacroAgent()
        result = await agent.process_input({'timeframes': ['1mo', '3mo']})
        print("MacroAgent Test Result:")
        print(f"Selected sectors: {len(result.get('selected_sectors', []))}")
        for sector in result.get('selected_sectors', []):
            print(f"  {sector['ticker']}: {sector['name']} (score: {sector['score']:.3f})")

    asyncio.run(test_macro_agent())